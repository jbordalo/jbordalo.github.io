
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"João Bordalo is a Computer Science PhD Student at NOVA School of Science and Technology. Currently researching the applications of machine learning for the self-management of complex distributed systems.\n","date":1715817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"João Bordalo is a Computer Science PhD Student at NOVA School of Science and Technology. Currently researching the applications of machine learning for the self-management of complex distributed systems.","tags":null,"title":"João Bordalo","type":"authors"},{"authors":["João Bordalo","Vasco Ramos","Rodrigo Valério","Diogo Glória-Silva","Yonatan Bitton","Michal Yarom","Idan Szpektor","João Magalhães"],"categories":null,"content":"","date":1715817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715817600,"objectID":"d1f727c4621ad59dc98fcf9254322aec","permalink":"https://jbordalo.github.io/publication/sequence-coherence/","publishdate":"2024-05-16T00:00:00Z","relpermalink":"/publication/sequence-coherence/","section":"publication","summary":"Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6% of the cases against 26.6\\% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across the sequence of visual illustrations.","tags":[],"title":"Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks","type":"publication"},{"authors":["João Bordalo"],"categories":null,"content":"","date":1703116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703116800,"objectID":"5f1dc8c1d74097cc45af1f9ed2482e5f","permalink":"https://jbordalo.github.io/publication/visual_dialogue_for_open_tasks/","publishdate":"2023-12-21T00:00:00Z","relpermalink":"/publication/visual_dialogue_for_open_tasks/","section":"publication","summary":"Visual Dialogue is a task requiring an AI agent to hold dialogue with humans in natural, conversational language about visual content. It is a challenging task, requiring a high level of understanding about both the visual world and natural language. The open nature of conversational agents further increases the complexity of this task. This task brings together the two main fields of AI and, being sufficiently detached from typical downstream tasks, serves as a general test of machine intelligence. In addition to the technical challenge, it is also an impactful application of AI, as it can help users when interacting with systems, improving their experience. In the context of this work, we propose to enrich the multimodal aspect of a task assistant, in two ways: 1) Dialogue Video Moment Retrieval: We will allow users to navigate through videos by voice. We will extract the video’s most relevant frames, create useful data about these frames, and index the data, so it can later be retrieved; 2) Task-Grounded Image Sequence Synthesis: We will use Image Synthesis models to illustrate task steps, with an emphasis on sequence coherence.","tags":[],"title":"Visual Dialogue for Open Tasks","type":"publication"},{"authors":["Rafael Ferreira","Diogo Tavares","Diogo Silva","Rodrigo Valério","João Bordalo","Inês Simões","Vasco Ramos","David Semedo","João Magalhães"],"categories":null,"content":"","date":1696291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696291200,"objectID":"7281f822c28e96386a9bf0caa6c08ab2","permalink":"https://jbordalo.github.io/publication/twiz2023/","publishdate":"2023-10-03T00:00:00Z","relpermalink":"/publication/twiz2023/","section":"publication","summary":"In this report, we describe the vision, challenges, and scientific contributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot Challenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal, knowledgeable, and engaging assistant that can guide users towards the successful completion of complex manual tasks. To achieve this, we focus our efforts on three main research questions: (1) Humanly-Shaped Conversations, by providing information in a knowledgeable way; (2) Multimodal Stimulus, making use of various modalities including voice, images, and videos; and (3) Zero-shot Conversational Flows, to improve the robustness of the interaction to unseen scenarios. TWIZ is an assistant capable of supporting a wide range of tasks, with several innovative features such as creative cooking, video navigation through voice, and the robust TWIZ-LLM, a Large Language Model trained for dialoguing about complex manual tasks. Given ratings and feedback provided by users, we observed that TWIZ bot is an effective and robust system, capable of guiding users through tasks while providing several multimodal stimuli.","tags":[],"title":"TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus","type":"publication"},{"authors":["Rodrigo Valério","João Bordalo","Michal Yarom","Yonatan Bitton","Idan Szpektor","João Magalhães"],"categories":null,"content":"","date":1684886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684886400,"objectID":"4e36d540d08ee0708ff3a5dcb36de988","permalink":"https://jbordalo.github.io/publication/nl2vi/","publishdate":"2023-05-24T00:00:00Z","relpermalink":"/publication/nl2vi/","section":"publication","summary":"Text to image generation methods (T2I) are widely popular in generating art and other creative artifacts. While visual hallucinations can be a positive factor in scenarios where creativity is appreciated, such artifacts are poorly suited for cases where the generated image needs to be grounded in complex natural language without explicit visual elements. In this paper, we propose to strengthen the consistency property of T2I methods in the presence of natural complex language, which often breaks the limits of T2I methods by including non-visual information, and textual elements that require knowledge for accurate generation. To address these phenomena, we propose a Natural Language to Verified Image generation approach (NL2VI) that converts a natural prompt into a visual prompt, which is more suitable for image generation. A T2I model then generates an image for the visual prompt, which is then verified with VQA algorithms. Experimentally, aligning natural prompts with image generation can improve the consistency of the generated images by up to 11% over the state of the art. Moreover, improvements can generalize to challenging domains like cooking and DIY tasks, where the correctness of the generated image is crucial to illustrate actions.","tags":[],"title":"Transferring Visual Attributes from Natural Language to Verified Image Generation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://jbordalo.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]